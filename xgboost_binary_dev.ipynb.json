{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "complete_data_path = \"data/complete_data/hunt4_complete.csv\"\n",
    "mi_data_path = \"data/mi_data\"\n",
    "results_path = \"data/results\"\n",
    "pred_path = \"data/pred_values\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "os.makedirs(pred_path, exist_ok=True)\n",
    "\n",
    "# Define feature groups\n",
    "categorical_vars = [\"maritalstat\", \"education\", \"health\", \"satlife\", \"painintens\", \"smoking\", \"alcofreq\", \"worktype\", \"cost_category\"]\n",
    "binary_vars = [\"sex\", \"armpain\", \"neckpain\", \"backuppain\", \"lumbarpain\", \"hiplegpain\", \"specconsult\", \"hospadmiss\", \"altcons\", \"headache\", \"workshift\"]\n",
    "continuous_vars = [\"age\", \"bmi\", \"pulse\", \"bpsyst\", \"bpdia\", \"waistcirc\", \"hipcirc\", \"actindex\", \"hadsdep\", \"hadsanx\", \"icpc_morbidity_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameter space for XGBoost\n",
    "param_space = {\n",
    "    \"predictor__learning_rate\": Real(0.0001, 0.1, prior=\"log-uniform\"),\n",
    "    \"predictor__max_depth\": Categorical([1, 2, 4, 8]),\n",
    "    \"predictor__n_estimators\": Categorical([100, 500]),\n",
    "    \"predictor__gamma\": Categorical([0, 1, 5, 10]),\n",
    "    \"predictor__min_child_weight\": Categorical([1, 5, 10]),\n",
    "    \"predictor__colsample_bytree\": Real(0.1, 0.8),\n",
    "    \"predictor__subsample\": Categorical([0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "    \"predictor__reg_alpha\": Categorical([0, 1, 5, 10, 15, 20]),\n",
    "    \"predictor__reg_lambda\": Categorical([1, 5, 10, 15, 20])\n",
    "}\n",
    "\n",
    "# Step 1: Parameter Tuning on Complete Data\n",
    "print(\"Performing Bayesian optimization for parameter tuning on complete data...\")\n",
    "complete_data = pd.read_csv(complete_data_path)\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "ids = complete_data[\"lopenr\"]  # ID column\n",
    "fold = complete_data[\"fold\"]  # Cluster variable\n",
    "y_complete = complete_data[\"high_cost_point\"]  # Outcome variable\n",
    "X_complete = complete_data.drop(columns=[\"lopenr\", \"fold\", \"high_cost_point\", \"persistent\"])\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False, drop=\"first\"), categorical_vars)\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Keep numerical columns as is\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('predictor', XGBClassifier(objective=\"binary:logistic\", random_state=42))\n",
    "])\n",
    "\n",
    "# Bayesian optimization\n",
    "opt = BayesSearchCV(\n",
    "    estimator=pipeline,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=50,  # Number of parameter configurations to test\n",
    "    cv=5,  # Cross-validation within the dataset\n",
    "    scoring=\"roc_auc\",\n",
    "    random_state=42,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "opt.fit(X_complete, y_complete)\n",
    "best_params = opt.best_params_\n",
    "\n",
    "# Save the best parameters\n",
    "print(\"Best Parameters from Complete Data:\", best_params)\n",
    "pd.DataFrame([best_params]).to_csv(os.path.join(results_path, \"best_params_complete.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(complete_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_data[\"high_cost_point\"].value_counts())\n",
    "print(complete_data[\"high_cost_point\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the best parameters\n",
    "best_params_file = os.path.join(results_path, \"best_params_complete.csv\")\n",
    "best_params = pd.read_csv(best_params_file).iloc[0].to_dict()\n",
    "\n",
    "# Ensure integer parameters are correctly cast\n",
    "for param in ['max_depth', 'n_estimators', 'min_child_weight']:\n",
    "    if f'predictor__{param}' in best_params:\n",
    "        best_params[f'predictor__{param}'] = int(best_params[f'predictor__{param}'])\n",
    "\n",
    "# Define the pipeline again\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False, drop=\"first\", handle_unknown=\"ignore\"), categorical_vars)\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Keep numerical columns as is\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('predictor', XGBClassifier(objective=\"binary:logistic\", random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the best parameters to the pipeline\n",
    "pipeline.set_params(**best_params)\n",
    "\n",
    "# Initialize a list to store the average performance across datasets and all fold results\n",
    "performance_results = []  # For xgboost_iecv_mean.csv\n",
    "all_fold_results = []  # For storing fold-level results across datasets\n",
    "fold_results_all = []\n",
    "\n",
    "# Function to calculate calibration metrics\n",
    "def calculate_calibration(y_true, y_pred):\n",
    "    slope = np.polyfit(y_pred, y_true, 1)[0]\n",
    "    citl = np.mean(y_true) - np.mean(y_pred)\n",
    "    return slope, citl\n",
    "\n",
    "# Loop through all imputed datasets\n",
    "for i in range(1, 21): \n",
    "    print(f\"Processing imputed dataset {i}/20...\")\n",
    "    file_path = os.path.join(mi_data_path, f\"hunt4_mi{i}.csv\")\n",
    "    dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Prepare dataset\n",
    "    ids = dataset[\"lopenr\"]\n",
    "    fold = dataset[\"fold\"]\n",
    "    y = dataset[\"high_cost_point\"]\n",
    "    X = dataset.drop(columns=[\"lopenr\", \"fold\", \"high_cost_point\", \"persistent\"])\n",
    "\n",
    "    # Initialize storage for fold predictions and fold performance\n",
    "    all_fold_predictions = []\n",
    "    fold_results = []\n",
    "\n",
    "    # Cross-validation\n",
    "    for fold_num in range(1, 6): \n",
    "        print(f\"  Fitting fold {fold_num}/5 for imputed dataset {i}...\")\n",
    "        train_idx = fold != fold_num\n",
    "        val_idx = fold == fold_num\n",
    "\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Fit and predict using the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Collect predictions for this fold\n",
    "        fold_predictions = pd.DataFrame({\n",
    "            \"ID\": ids[val_idx],\n",
    "            \"True\": y_val,\n",
    "            \"Predicted\": y_pred,\n",
    "            \"Fold\": [fold_num] * len(y_val),\n",
    "        })\n",
    "        all_fold_predictions.append(fold_predictions)\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        calibration_slope, citl = calculate_calibration(y_val, y_pred)\n",
    "\n",
    "        # Store fold performance\n",
    "        fold_results.append({\n",
    "            \"Fold\": fold_num,\n",
    "            \"AUC\": auc,\n",
    "            \"Calibration Slope\": calibration_slope,\n",
    "            \"CITL\": citl,\n",
    "            \"Imputation\": i,\n",
    "        })\n",
    "\n",
    "    # Combine predictions for the dataset\n",
    "    all_fold_predictions_df = pd.concat(all_fold_predictions, ignore_index=True)\n",
    "    all_fold_predictions_df.to_csv(\n",
    "        os.path.join(pred_path, f\"pred_values_mi{i}.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # Save performance results for this dataset\n",
    "    fold_results_df = pd.DataFrame(fold_results)\n",
    "    fold_results_df.to_csv(\n",
    "        os.path.join(results_path, f\"xgboost_iecv_mi{i}.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # Append fold results for global averaging\n",
    "    fold_results_all.extend(fold_results)\n",
    "\n",
    "    # Save the trained pipeline\n",
    "    if i == 1:\n",
    "        saved_model_path = \"savedmodels\"\n",
    "        os.makedirs(saved_model_path, exist_ok=True)\n",
    "        joblib.dump(pipeline, os.path.join(saved_model_path, \"xgboost_pipeline.joblib\"))\n",
    "\n",
    "# Combine fold-level results across all datasets\n",
    "fold_results_all_df = pd.DataFrame(fold_results_all)\n",
    "\n",
    "# Compute overall averages across all folds and datasets\n",
    "overall_avg = fold_results_all_df.groupby(\"Fold\", as_index=False).mean(numeric_only=True)\n",
    "if \"Imputation\" in overall_avg.columns:\n",
    "    overall_avg = overall_avg.drop(columns=[\"Imputation\"])\n",
    "overall_avg.loc[len(overall_avg)] = fold_results_all_df.mean(numeric_only=True).to_dict()\n",
    "overall_avg.loc[len(overall_avg) - 1, \"Fold\"] = \"Overall Average\"\n",
    "\n",
    "# Save overall performance metrics\n",
    "overall_avg.to_csv(os.path.join(results_path, \"xgboost_iecv_mean.csv\"), index=False)\n",
    "\n",
    "print(\"CV and model saving completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the results file\n",
    "results_file = \"data/results/xgboost_iecv_mean.csv\"\n",
    "\n",
    "# Read and display the CSV\n",
    "results_df = pd.read_csv(results_file, sep=\",\")\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
